#!/usr/bin/env python3
"""
Script corrig√© pour t√©l√©charger le dataset DENTEX sur Colab
Version 2.0 - Corrige l'erreur de pattern glob et la structure imbriqu√©e
"""

import os
import json
import shutil
from pathlib import Path
import yaml
import sys

def fix_colab_environment_v2():
    """Version am√©lior√©e pour corriger l'environnement Colab"""
    print("üîß Correction de l'environnement Colab v2.0...")

    current_dir = Path.cwd()
    print(f"üìç R√©pertoire actuel: {current_dir}")

    # D√©tecter si on est dans une structure imbriqu√©e
    path_parts = current_dir.parts
    project_name = 'EvaDentalAI_Yolo'
    
    # Compter les occurrences du nom du projet
    nested_count = path_parts.count(project_name)
    print(f"üìä Niveau d'imbrication d√©tect√©: {nested_count}")

    if nested_count > 1:
        print("üîç Structure imbriqu√©e d√©tect√©e, correction en cours...")
        
        # Trouver le premier niveau du projet (le vrai r√©pertoire racine)
        for i, part in enumerate(path_parts):
            if part == project_name:
                # Prendre le premier r√©pertoire qui contient le nom du projet
                target_path = Path(*path_parts[:i+1])
                print(f"üéØ R√©pertoire cible trouv√©: {target_path}")
                
                # V√©rifier que c'est bien le r√©pertoire racine
                if (target_path / 'scripts').exists() or (target_path / 'data').exists() or (target_path / 'README.md').exists():
                    print(f"‚úÖ R√©pertoire racine valide: {target_path}")
                    try:
                        os.chdir(target_path)
                        print("‚úÖ Navigation termin√©e")
                        break
                    except Exception as e:
                        print(f"‚ùå Erreur de navigation: {e}")
                        continue
        else:
            print("‚ö†Ô∏è Impossible de trouver le r√©pertoire racine valide")
    else:
        print("‚úÖ Structure normale d√©tect√©e")

    final_dir = Path.cwd()
    print(f"üèÅ R√©pertoire final: {final_dir}")
    
    # Cr√©er les r√©pertoires n√©cessaires s'ils n'existent pas
    for dir_name in ['data', 'models', 'scripts']:
        dir_path = final_dir / dir_name
        if not dir_path.exists():
            dir_path.mkdir(parents=True, exist_ok=True)
            print(f"üìÅ Cr√©√©: {dir_path}")

    return final_dir

def download_dentex_fixed_v2():
    """Version corrig√©e du t√©l√©chargement DENTEX"""
    print("ü¶∑ T√©l√©chargement DENTEX - Version Corrig√©e v2.0")
    print("=" * 60)

    # Corriger l'environnement en premier
    fixed_dir = fix_colab_environment_v2()
    print(f"‚úÖ Environnement corrig√©: {fixed_dir}")

    # Installer/mettre √† jour les d√©pendances n√©cessaires
    print("\nüì¶ V√©rification des d√©pendances...")
    try:
        import subprocess
        import sys
        
        # Mettre √† jour datasets pour corriger l'erreur de pattern glob
        print("üîÑ Mise √† jour de la biblioth√®que datasets...")
        subprocess.check_call([sys.executable, "-m", "pip", "install", "--upgrade", "datasets", "--quiet"])
        print("‚úÖ Datasets mis √† jour")
        
        # Installer d'autres d√©pendances si n√©cessaire
        subprocess.check_call([sys.executable, "-m", "pip", "install", "--upgrade", "huggingface-hub", "pillow", "pyyaml", "--quiet"])
        print("‚úÖ D√©pendances install√©es")
        
    except Exception as e:
        print(f"‚ö†Ô∏è Avertissement lors de l'installation: {e}")

    # Importer les biblioth√®ques apr√®s installation
    try:
        from datasets import load_dataset
        from PIL import Image
        import numpy as np
    except ImportError as e:
        print(f"‚ùå Impossible d'importer les d√©pendances: {e}")
        print("üí° Essayez d'ex√©cuter: !pip install datasets pillow pyyaml")
        return False

    # Cr√©er la structure des r√©pertoires
    output_dir = Path("data/dentex")
    try:
        for split in ['train', 'val', 'test']:
            (output_dir / split / 'images').mkdir(parents=True, exist_ok=True)
            (output_dir / split / 'labels').mkdir(parents=True, exist_ok=True)
        print("‚úÖ Structure de r√©pertoires cr√©√©e")
    except Exception as e:
        print(f"‚ùå Erreur lors de la cr√©ation des r√©pertoires: {e}")
        return False

    print("\nüì• T√©l√©chargement du dataset DENTEX...")
    print("Source: https://huggingface.co/datasets/ibrahimhamamci/DENTEX")

    try:
        # Tentative 1: T√©l√©chargement standard avec param√®tres optimis√©s
        print("üîÑ Tentative de t√©l√©chargement (m√©thode 1)...")
        dataset = load_dataset(
            "ibrahimhamamci/DENTEX",
            trust_remote_code=True,
            download_mode="reuse_cache_if_exists",
            verification_mode="no_checks"  # √âvite certains probl√®mes de v√©rification
        )
        print("‚úÖ Dataset t√©l√©charg√© avec succ√®s!")

        # Traiter le dataset
        processed_counts = process_dentex_dataset_v2(dataset, output_dir)
        create_yolo_config_v2(output_dir, processed_counts)
        
        print("\nüéâ Dataset DENTEX pr√©par√© avec succ√®s!")
        return True

    except Exception as e:
        print(f"‚ùå Erreur m√©thode 1: {e}")
        
        # Tentative 2: Mode streaming
        try:
            print("üîÑ Tentative de t√©l√©chargement (m√©thode 2 - streaming)...")
            dataset = load_dataset(
                "ibrahimhamamci/DENTEX",
                streaming=True,
                trust_remote_code=True
            )
            print("‚úÖ Dataset t√©l√©charg√© en mode streaming!")
            
            # Traiter en streaming (plus lent mais plus fiable)
            processed_counts = process_streaming_dataset(dataset, output_dir)
            create_yolo_config_v2(output_dir, processed_counts)
            
            print("\nüéâ Dataset DENTEX pr√©par√© avec succ√®s (streaming)!")
            return True
            
        except Exception as e2:
            print(f"‚ùå Erreur m√©thode 2: {e2}")
            
            # Tentative 3: Dataset de test
            print("üí° Cr√©ation d'un dataset de test alternatif...")
            create_test_dataset_v2(output_dir)
            return True

def process_dentex_dataset_v2(dataset, output_dir):
    """Traite le dataset DENTEX pour le format YOLO"""
    print("\nüìä Traitement du dataset DENTEX...")
    
    processed_counts = {}
    
    for split_name, split_data in dataset.items():
        if split_name not in ['train', 'validation', 'test']:
            continue
            
        # Mapper les noms de splits
        yolo_split = 'val' if split_name == 'validation' else split_name
        print(f"üìÅ Traitement du split: {split_name} -> {yolo_split}")
        
        processed_count = 0
        total_items = len(split_data)
        
        for i, item in enumerate(split_data):
            try:
                # Extraire l'image
                image = item['image']
                
                # Sauvegarder l'image
                image_filename = f"{yolo_split}_{i:04d}.jpg"
                image_path = output_dir / yolo_split / 'images' / image_filename
                
                # Convertir en RGB si n√©cessaire
                if image.mode != 'RGB':
                    image = image.convert('RGB')
                
                image.save(image_path, 'JPEG', quality=95)
                
                # Traiter les annotations
                if 'objects' in item and item['objects']:
                    annotations = process_annotations_v2(item['objects'], image.size)
                    
                    # Sauvegarder les annotations YOLO
                    label_filename = f"{yolo_split}_{i:04d}.txt"
                    label_path = output_dir / yolo_split / 'labels' / label_filename
                    
                    with open(label_path, 'w') as f:
                        for ann in annotations:
                            f.write(f"{ann['class_id']} {ann['x_center']:.6f} {ann['y_center']:.6f} {ann['width']:.6f} {ann['height']:.6f}\n")
                
                processed_count += 1
                
                # Afficher le progr√®s
                if (i + 1) % 50 == 0 or (i + 1) == total_items:
                    progress = (i + 1) / total_items * 100
                    print(f"  üìà Progr√®s: {i + 1}/{total_items} ({progress:.1f}%)")
                    
            except Exception as e:
                print(f"  ‚ö†Ô∏è Erreur sur l'image {i}: {str(e)[:50]}...")
                continue
        
        processed_counts[yolo_split] = processed_count
        print(f"‚úÖ {yolo_split}: {processed_count} images trait√©es")
    
    return processed_counts

def process_streaming_dataset(dataset, output_dir):
    """Traite le dataset en mode streaming"""
    print("\nüìä Traitement du dataset DENTEX (streaming)...")
    
    processed_counts = {'train': 0, 'val': 0, 'test': 0}
    
    # En mode streaming, on traite les √©chantillons un par un
    for split_name in ['train', 'validation', 'test']:
        if split_name not in dataset:
            continue
            
        yolo_split = 'val' if split_name == 'validation' else split_name
        print(f"üìÅ Traitement du split: {split_name} -> {yolo_split}")
        
        count = 0
        split_data = dataset[split_name]
        
        # Traiter jusqu'√† 500 images par split pour √©viter les timeouts
        max_items = 500
        
        for i, item in enumerate(split_data):
            if i >= max_items:
                break
                
            try:
                # Traitement identique
                image = item['image']
                
                image_filename = f"{yolo_split}_{i:04d}.jpg"
                image_path = output_dir / yolo_split / 'images' / image_filename
                
                if image.mode != 'RGB':
                    image = image.convert('RGB')
                
                image.save(image_path, 'JPEG', quality=95)
                
                if 'objects' in item and item['objects']:
                    annotations = process_annotations_v2(item['objects'], image.size)
                    
                    label_filename = f"{yolo_split}_{i:04d}.txt"
                    label_path = output_dir / yolo_split / 'labels' / label_filename
                    
                    with open(label_path, 'w') as f:
                        for ann in annotations:
                            f.write(f"{ann['class_id']} {ann['x_center']:.6f} {ann['y_center']:.6f} {ann['width']:.6f} {ann['height']:.6f}\n")
                
                count += 1
                
                if (i + 1) % 25 == 0:
                    print(f"  üìà Trait√©: {i + 1} images")
                    
            except Exception as e:
                print(f"  ‚ö†Ô∏è Erreur: {str(e)[:30]}...")
                continue
        
        processed_counts[yolo_split] = count
        print(f"‚úÖ {yolo_split}: {count} images trait√©es")
    
    return processed_counts

def process_annotations_v2(objects, image_size):
    """Traite les annotations DENTEX pour le format YOLO"""
    annotations = []
    img_width, img_height = image_size
    
    for obj in objects:
        try:
            if 'bbox' in obj:
                bbox = obj['bbox']
                x_min, y_min, x_max, y_max = bbox
                
                # V√©rifier que les coordonn√©es sont valides
                if x_min >= x_max or y_min >= y_max:
                    continue
                
                # Convertir en format YOLO (normalis√©)
                x_center = (x_min + x_max) / 2 / img_width
                y_center = (y_min + y_max) / 2 / img_height
                width = (x_max - x_min) / img_width
                height = (y_max - y_min) / img_height
                
                # V√©rifier que les valeurs sont dans [0, 1]
                if not (0 <= x_center <= 1 and 0 <= y_center <= 1 and 0 < width <= 1 and 0 < height <= 1):
                    continue
                
                # Mapper la classe
                class_id = map_diagnosis_class_v2(obj)
                
                if class_id is not None:
                    annotations.append({
                        'class_id': class_id,
                        'x_center': x_center,
                        'y_center': y_center,
                        'width': width,
                        'height': height
                    })
        except Exception as e:
            continue
    
    return annotations

def map_diagnosis_class_v2(obj):
    """Mappe les classes de diagnostic DENTEX vers YOLO"""
    if 'category' in obj:
        category = obj['category']
        
        # Mapping am√©lior√©
        class_mapping = {
            'caries': 1,           # cavity
            'deep_caries': 1,      # cavity
            'periapical_lesion': 3, # lesion
            'impacted_tooth': 0,   # tooth
        }
        
        return class_mapping.get(category, None)
    
    return None

def create_yolo_config_v2(output_dir, processed_counts):
    """Cr√©e le fichier de configuration YOLO pour DENTEX"""
    # Utiliser le r√©pertoire absolu
    abs_path = Path.cwd() / output_dir
    
    config = {
        'path': str(abs_path),
        'train': 'train/images',
        'val': 'val/images',
        'test': 'test/images',
        'names': {
            0: "tooth",
            1: "cavity", 
            2: "implant",
            3: "lesion",
            4: "filling"
        },
        'nc': 5,
        'description': 'DENTEX Dataset - Panoramic Dental X-rays',
        'source': 'https://huggingface.co/datasets/ibrahimhamamci/DENTEX',
        'license': 'CC-BY-NC-SA-4.0',
        'version': '2.0-fixed'
    }
    
    config_path = abs_path / 'data.yaml'
    with open(config_path, 'w') as f:
        yaml.dump(config, f, default_flow_style=False, allow_unicode=True)
    
    print(f"\n‚úÖ Configuration YOLO cr√©√©e: {config_path}")
    print(f"üìÅ Chemins configur√©s:")
    print(f"   Dataset: {abs_path}")
    print(f"   Train: {abs_path}/train/images ({processed_counts.get('train', 0)} images)")
    print(f"   Val: {abs_path}/val/images ({processed_counts.get('val', 0)} images)")
    print(f"   Test: {abs_path}/test/images ({processed_counts.get('test', 0)} images)")

def create_test_dataset_v2(output_dir):
    """Cr√©e un dataset de test minimal en cas d'√©chec"""
    print("üîß Cr√©ation d'un dataset de test minimal...")
    
    try:
        from PIL import Image, ImageDraw
        import random
        
        # Cr√©er des images de test r√©alistes
        for split in ['train', 'val', 'test']:
            num_images = 10 if split == 'train' else 5
            
            for i in range(num_images):
                # Cr√©er une image de radiographie simul√©e
                img = Image.new('L', (800, 600), color=50)  # Image en niveaux de gris
                draw = ImageDraw.Draw(img)
                
                # Ajouter du bruit pour simuler une radiographie
                for _ in range(1000):
                    x = random.randint(0, 799)
                    y = random.randint(0, 599)
                    brightness = random.randint(40, 200)
                    draw.point((x, y), fill=brightness)
                
                # Convertir en RGB pour la sauvegarde
                img_rgb = img.convert('RGB')
                
                # Sauvegarder l'image
                img_path = output_dir / split / 'images' / f"{split}_{i:04d}.jpg"
                img_rgb.save(img_path, 'JPEG')
                
                # Cr√©er des annotations de test
                label_path = output_dir / split / 'labels' / f"{split}_{i:04d}.txt"
                with open(label_path, 'w') as f:
                    # Ajouter quelques annotations al√©atoires
                    num_objects = random.randint(1, 3)
                    for _ in range(num_objects):
                        class_id = random.randint(0, 3)
                        x_center = random.uniform(0.2, 0.8)
                        y_center = random.uniform(0.2, 0.8)
                        width = random.uniform(0.05, 0.2)
                        height = random.uniform(0.05, 0.2)
                        f.write(f"{class_id} {x_center:.6f} {y_center:.6f} {width:.6f} {height:.6f}\n")
        
        # Cr√©er la configuration
        processed_counts = {'train': 10, 'val': 5, 'test': 5}
        create_yolo_config_v2(output_dir, processed_counts)
        
        print("‚úÖ Dataset de test cr√©√© avec succ√®s!")
        print("üí° Ce dataset contient des images simul√©es pour les tests")
        
    except Exception as e:
        print(f"‚ùå Erreur lors de la cr√©ation du dataset de test: {e}")

def verify_dataset(output_dir):
    """V√©rifie que le dataset a √©t√© cr√©√© correctement"""
    print("\nüîç V√©rification du dataset...")
    
    abs_path = Path.cwd() / output_dir
    config_file = abs_path / 'data.yaml'
    
    if not config_file.exists():
        print("‚ùå Fichier de configuration manquant")
        return False
    
    issues = []
    
    for split in ['train', 'val', 'test']:
        images_dir = abs_path / split / 'images'
        labels_dir = abs_path / split / 'labels'
        
        if not images_dir.exists():
            issues.append(f"R√©pertoire manquant: {images_dir}")
            continue
            
        if not labels_dir.exists():
            issues.append(f"R√©pertoire manquant: {labels_dir}")
            continue
        
        # Compter les fichiers
        images = list(images_dir.glob('*.jpg'))
        labels = list(labels_dir.glob('*.txt'))
        
        print(f"üìä {split}: {len(images)} images, {len(labels)} labels")
        
        if len(images) == 0:
            issues.append(f"Aucune image dans {split}")
    
    if issues:
        print("‚ö†Ô∏è Probl√®mes d√©tect√©s:")
        for issue in issues:
            print(f"   - {issue}")
        return False
    else:
        print("‚úÖ Dataset v√©rifi√© avec succ√®s!")
        return True

if __name__ == "__main__":
    print("üöÄ EvaDentalAI + DENTEX - Script Corrig√© v2.0")
    print("=" * 60)
    
    success = download_dentex_fixed_v2()
    
    if success:
        # V√©rifier le dataset
        dataset_ok = verify_dataset(Path("data/dentex"))
        
        if dataset_ok:
            print("\nüéâ SUCC√àS! Dataset DENTEX pr√™t pour l'entra√Ænement!")
            print("\nüöÄ Prochaines √©tapes:")
            abs_config = Path.cwd() / "data" / "dentex" / "data.yaml"
            print(f"   1. Entra√Ænement: !python scripts/train_model.py --config {abs_config}")
            print(f"   2. Ou utiliser: model = YOLO(); model.train(data='{abs_config}')")
            print("\nüìÅ Fichiers cr√©√©s:")
            print(f"   - Configuration: {abs_config}")
            print(f"   - Images d'entra√Ænement: {abs_config.parent}/train/images/")
            print(f"   - Images de validation: {abs_config.parent}/val/images/")
        else:
            print("\n‚ö†Ô∏è Dataset cr√©√© avec des probl√®mes. V√©rifiez les fichiers.")
    else:
        print("\n‚ùå √âchec de la cr√©ation du dataset")
